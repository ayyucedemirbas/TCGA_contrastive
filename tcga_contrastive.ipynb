{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyOmupt8XnRGWwBYNd3KPF9z",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ayyucedemirbas/TCGA_contrastive/blob/main/tcga_contrastive.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ot6ElwtVzAnU",
        "outputId": "71aab6b6-7d07-4644-bf30-4bc9e25a8b9e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'MLOmics'...\n",
            "remote: Enumerating objects: 435, done.\u001b[K\n",
            "remote: Total 435 (delta 0), reused 0 (delta 0), pack-reused 435 (from 1)\u001b[K\n",
            "Receiving objects: 100% (435/435), 57.17 KiB | 5.72 MiB/s, done.\n",
            "Resolving deltas: 100% (56/56), done.\n",
            "Filtering content: 100% (235/235), 8.83 GiB | 53.76 MiB/s, done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://huggingface.co/datasets/AIBIC/MLOmics"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torch_geometric"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uAIajh34zKiu",
        "outputId": "e123952b-5972-434d-c9b0-221a2ebc840f"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting torch_geometric\n",
            "  Downloading torch_geometric-2.7.0-py3-none-any.whl.metadata (63 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/63.7 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.7/63.7 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: aiohttp in /usr/local/lib/python3.12/dist-packages (from torch_geometric) (3.13.1)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch_geometric) (2025.3.0)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch_geometric) (3.1.6)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from torch_geometric) (2.0.2)\n",
            "Requirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.12/dist-packages (from torch_geometric) (5.9.5)\n",
            "Requirement already satisfied: pyparsing in /usr/local/lib/python3.12/dist-packages (from torch_geometric) (3.2.5)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from torch_geometric) (2.32.4)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from torch_geometric) (4.67.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.12/dist-packages (from torch_geometric) (3.6.0)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->torch_geometric) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->torch_geometric) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->torch_geometric) (25.4.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp->torch_geometric) (1.8.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp->torch_geometric) (6.7.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->torch_geometric) (0.4.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->torch_geometric) (1.22.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch_geometric) (3.0.3)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->torch_geometric) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->torch_geometric) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->torch_geometric) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->torch_geometric) (2025.10.5)\n",
            "Requirement already satisfied: typing-extensions>=4.2 in /usr/local/lib/python3.12/dist-packages (from aiosignal>=1.4.0->aiohttp->torch_geometric) (4.15.0)\n",
            "Downloading torch_geometric-2.7.0-py3-none-any.whl (1.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m38.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: torch_geometric\n",
            "Successfully installed torch_geometric-2.7.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch_geometric.nn import GATConv, global_mean_pool, global_max_pool\n",
        "from torch_geometric.data import Data, Batch\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "from sklearn.metrics import accuracy_score, f1_score, roc_auc_score\n",
        "import os\n",
        "from tqdm import tqdm\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ],
      "metadata": {
        "id": "6ukWgpfkzNQc"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(42)\n",
        "np.random.seed(42)"
      ],
      "metadata": {
        "id": "9NDTHKMszTqs"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Config:\n",
        "    # Dataset paths\n",
        "    dataset_root = 'MLOmics/Main_Dataset/Classification_datasets'\n",
        "    cancer_type = 'GS-BRCA'  # GS-COAD, GS-GBM, GS-LGG, GS-OV\n",
        "    data_version = 'Top'  # 'Original', 'Aligned', 'Top'\n",
        "\n",
        "\n",
        "    hidden_dim = 256\n",
        "    gat_heads = 4\n",
        "    num_gat_layers = 3\n",
        "    dropout = 0.3\n",
        "    projection_dim = 128\n",
        "\n",
        "\n",
        "    batch_size = 2\n",
        "    learning_rate = 0.001\n",
        "    num_epochs = 100\n",
        "    patience = 15\n",
        "\n",
        "    temperature = 0.07\n",
        "    contrastive_weight = 0.5\n",
        "\n",
        "    top_k_neighbors = 7\n",
        "\n",
        "\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "    save_dir = 'models'\n",
        "    model_name = f'best_gat_contrastive_{cancer_type}.pth'\n",
        "\n",
        "\n",
        "\n",
        "config = Config()\n",
        "os.makedirs(config.save_dir, exist_ok=True)"
      ],
      "metadata": {
        "id": "UfCiXql3zVk4"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TCGADataLoader:\n",
        "    def __init__(self, config):\n",
        "        self.config = config\n",
        "        self.data_path = os.path.join(\n",
        "            config.dataset_root,\n",
        "            config.cancer_type,\n",
        "            config.data_version\n",
        "        )\n",
        "        os.makedirs(self.data_path, exist_ok=False) if not os.path.exists(self.data_path) else None\n",
        "\n",
        "    def _read_csv_try(self, path, idx_col=0):\n",
        "        try:\n",
        "            df = pd.read_csv(path, index_col=idx_col)\n",
        "            return df\n",
        "        except Exception:\n",
        "            df = pd.read_csv(path, header=None)\n",
        "            return df\n",
        "\n",
        "    def load_data(self):\n",
        "        cancer_prefix = config.cancer_type.split('-')[1]\n",
        "        print(f\"Loading from: {self.data_path}\")\n",
        "\n",
        "        def path_for(name):\n",
        "            return os.path.join(self.data_path, name)\n",
        "\n",
        "        def _load_matrix(prefix, typ):\n",
        "            candidates = [\n",
        "                f\"{prefix}_{typ}_{config.data_version.lower()}.csv\",\n",
        "                f\"{prefix}_{typ}.csv\",\n",
        "                f\"{typ}_{prefix}.csv\",\n",
        "                f\"{prefix}_{typ}.CSV\"\n",
        "            ]\n",
        "            for fname in candidates:\n",
        "                p = path_for(fname)\n",
        "                if os.path.exists(p):\n",
        "                    try:\n",
        "                        return pd.read_csv(p, index_col=0)\n",
        "                    except Exception:\n",
        "                        return pd.read_csv(p, header=None)\n",
        "            raise FileNotFoundError(f\"None of candidate files found for {prefix} {typ}: {candidates}\")\n",
        "\n",
        "        cnv = _load_matrix(cancer_prefix, 'CNV')\n",
        "        methy = _load_matrix(cancer_prefix, 'Methy')\n",
        "        mrna = _load_matrix(cancer_prefix, 'mRNA')\n",
        "        mirna = _load_matrix(cancer_prefix, 'miRNA')\n",
        "\n",
        "        label_path_candidates = [\n",
        "            f\"{cancer_prefix}_label_num.csv\",\n",
        "            f\"{cancer_prefix}_label.csv\",\n",
        "            \"label.csv\",\n",
        "            \"labels.csv\"\n",
        "        ]\n",
        "        labels = None\n",
        "        for fname in label_path_candidates:\n",
        "            p = path_for(fname)\n",
        "            if os.path.exists(p):\n",
        "                try:\n",
        "                    labels = pd.read_csv(p, index_col=0)\n",
        "                except Exception:\n",
        "                    labels = pd.read_csv(p, header=None)\n",
        "                break\n",
        "        if labels is None:\n",
        "            raise FileNotFoundError(\"Labels file not found in expected locations\")\n",
        "\n",
        "        print(f\"Loaded - CNV: {cnv.shape}, Methy: {methy.shape}, mRNA: {mrna.shape}, miRNA: {mirna.shape}, Labels: {labels.shape}\")\n",
        "\n",
        "        label_count = None\n",
        "        if isinstance(labels, pd.DataFrame) and labels.shape[1] >= 1:\n",
        "            label_count = labels.shape[0]\n",
        "        else:\n",
        "            label_count = labels.shape[0]\n",
        "\n",
        "        def ensure_samples_rows(df, name):\n",
        "            if df.shape[0] == label_count:\n",
        "                print(f\"{name} already samples x features: {df.shape}\")\n",
        "                return df\n",
        "            elif df.shape[1] == label_count:\n",
        "                print(f\"Transposing {name} from (features x samples) to (samples x features): {df.shape} -> {df.T.shape}\")\n",
        "                return df.T\n",
        "            else:\n",
        "                sample_index_candidates = set(df.index.astype(str))\n",
        "                label_index_candidates = set()\n",
        "                if isinstance(labels, pd.DataFrame):\n",
        "                    for col in labels.columns:\n",
        "                        label_index_candidates |= set(labels[col].astype(str))\n",
        "                label_index_candidates |= set(labels.index.astype(str))\n",
        "\n",
        "                if len(sample_index_candidates & label_index_candidates) > 0:\n",
        "                    print(f\"Detected sample ID overlap between {name} index and labels; using current orientation: {df.shape}\")\n",
        "                    return df\n",
        "                if len(set(df.columns.astype(str)) & label_index_candidates) > 0:\n",
        "                    print(f\"Detected sample ID overlap between {name} columns and labels; transposing {name}\")\n",
        "                    return df.T\n",
        "\n",
        "                if df.shape[0] > df.shape[1]:\n",
        "                    print(f\"Heuristic transpose for {name} (rows > cols). Transposing {df.shape} -> {df.T.shape}\")\n",
        "                    return df.T\n",
        "\n",
        "                raise ValueError(f\"Cannot determine sample axis for {name}. df.shape={df.shape}, label_count={label_count}\")\n",
        "\n",
        "        cnv = ensure_samples_rows(cnv, 'CNV')\n",
        "        methy = ensure_samples_rows(methy, 'Methy')\n",
        "        mrna = ensure_samples_rows(mrna, 'mRNA')\n",
        "        mirna = ensure_samples_rows(mirna, 'miRNA')\n",
        "\n",
        "        print(f\"After ensure_samples_rows - CNV: {cnv.shape}, Methy: {methy.shape}, mRNA: {mrna.shape}, miRNA: {mirna.shape}\")\n",
        "\n",
        "        cnv.index = cnv.index.astype(str).str.strip()\n",
        "        methy.index = methy.index.astype(str).str.strip()\n",
        "        mrna.index = mrna.index.astype(str).str.strip()\n",
        "        mirna.index = mirna.index.astype(str).str.strip()\n",
        "\n",
        "        if isinstance(labels, pd.DataFrame) and labels.shape[1] == 0:\n",
        "            labels = labels.copy()\n",
        "            labels['label'] = labels.index.astype(str)\n",
        "        if isinstance(labels, pd.DataFrame) and labels.shape[1] == 1:\n",
        "            col = labels.columns[0]\n",
        "            if set(labels[col].astype(str)) & set(cnv.index):\n",
        "                labels = labels.set_index(col)\n",
        "                if labels.shape[1] == 0:\n",
        "                    labels['label'] = np.nan\n",
        "            else:\n",
        "                if len(labels) == len(cnv):\n",
        "                    labels.index = cnv.index\n",
        "                    labels = labels.rename(columns={col: 'label'})\n",
        "                else:\n",
        "                    if len(set(labels.index.astype(str)) & set(cnv.index)) == 0:\n",
        "                        labels = pd.read_csv(os.path.join(self.data_path, f\"{cancer_prefix}_label_num.csv\"), header=None)\n",
        "                        if labels.shape[0] == cnv.shape[0]:\n",
        "                            labels.index = cnv.index\n",
        "                            labels.columns = ['label']\n",
        "                        else:\n",
        "                            labels = pd.DataFrame({'label': np.zeros(len(cnv))}, index=cnv.index)\n",
        "        else:\n",
        "            if isinstance(labels, pd.DataFrame) and labels.shape[1] > 1:\n",
        "                numeric_cols = [c for c in labels.columns if pd.api.types.is_numeric_dtype(labels[c])]\n",
        "                if len(numeric_cols) >= 1:\n",
        "                    labels = labels[[numeric_cols[0]]]\n",
        "                    labels.columns = ['label']\n",
        "                else:\n",
        "                    if len(set(labels.index.astype(str)) & set(cnv.index)) == 0:\n",
        "                        labels = pd.DataFrame({'label': np.zeros(len(cnv))}, index=cnv.index)\n",
        "\n",
        "        labels.index = labels.index.astype(str).str.strip()\n",
        "        labels['label'] = labels.iloc[:, 0]\n",
        "        labels = labels[['label']]\n",
        "\n",
        "        print(f\"Labels processed: {labels.shape}\")\n",
        "\n",
        "        common_samples = list(set(cnv.index) & set(methy.index) & set(mrna.index) & set(mirna.index) & set(labels.index))\n",
        "\n",
        "        if len(common_samples) == 0:\n",
        "            print(list(cnv.index[:10]))\n",
        "            print(list(methy.index[:10]))\n",
        "            print(list(mrna.index[:10]))\n",
        "            print(list(mirna.index[:10]))\n",
        "            print(list(labels.index[:10]))\n",
        "            raise ValueError(\n",
        "                \"No common samples found across modalities. \"\n",
        "                \"Sample IDs don't match. Check the CSV file structure or sample naming conventions.\"\n",
        "            )\n",
        "\n",
        "        common_samples.sort()\n",
        "        print(f\"\\nFound {len(common_samples)} common samples\")\n",
        "\n",
        "        cnv = cnv.loc[common_samples].copy()\n",
        "        methy = methy.loc[common_samples].copy()\n",
        "        mrna = mrna.loc[common_samples].copy()\n",
        "        mirna = mirna.loc[common_samples].copy()\n",
        "        labels = labels.loc[common_samples].copy()\n",
        "\n",
        "        print(f\"\\nLabel types before encoding: {labels['label'].dtype}\")\n",
        "        print(f\"Unique labels: {labels['label'].unique()}\")\n",
        "\n",
        "        if labels['label'].dtype == 'object' or labels['label'].dtype.name == 'string':\n",
        "            print(\"Labels are categorical/strings. Encoding to numeric...\")\n",
        "            label_encoder = LabelEncoder()\n",
        "            labels['label'] = label_encoder.fit_transform(labels['label'])\n",
        "            print(f\"Label mapping: {dict(zip(label_encoder.classes_, label_encoder.transform(label_encoder.classes_)))}\")\n",
        "            self.label_encoder = label_encoder\n",
        "        else:\n",
        "            labels['label'] = labels['label'].astype(int)\n",
        "            self.label_encoder = None\n",
        "\n",
        "        print(f\"Labels after encoding: {labels['label'].dtype}, unique values: {labels['label'].unique()}\")\n",
        "\n",
        "        cnv = cnv.reset_index(drop=True)\n",
        "        methy = methy.reset_index(drop=True)\n",
        "        mrna = mrna.reset_index(drop=True)\n",
        "        mirna = mirna.reset_index(drop=True)\n",
        "        labels = labels.reset_index(drop=True)\n",
        "\n",
        "        cnv = cnv.fillna(cnv.mean())\n",
        "        methy = methy.fillna(methy.mean())\n",
        "        mrna = mrna.fillna(mrna.mean())\n",
        "        mirna = mirna.fillna(mirna.mean())\n",
        "\n",
        "        cnv = cnv.fillna(0)\n",
        "        methy = methy.fillna(0)\n",
        "        mrna = mrna.fillna(0)\n",
        "        mirna = mirna.fillna(0)\n",
        "\n",
        "        print(f\"Final data shapes - CNV: {cnv.shape}, Methy: {methy.shape}, mRNA: {mrna.shape}, miRNA: {mirna.shape}, Labels: {labels.shape}\")\n",
        "\n",
        "        return cnv, methy, mrna, mirna, labels\n",
        "\n",
        "    def create_train_test_split(self, cnv, methy, mrna, mirna, labels, test_size=0.2):\n",
        "        indices = np.arange(len(cnv))\n",
        "        y = labels['label'].values.ravel()\n",
        "        try:\n",
        "            train_idx, test_idx = train_test_split(\n",
        "                indices, test_size=test_size, random_state=42, stratify=y\n",
        "            )\n",
        "        except Exception:\n",
        "            train_idx, test_idx = train_test_split(\n",
        "                indices, test_size=test_size, random_state=42, stratify=None\n",
        "            )\n",
        "        return train_idx, test_idx"
      ],
      "metadata": {
        "id": "ACEqPno0zsYk"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class PatientGraphBuilder:\n",
        "\n",
        "    def __init__(self, top_k=10):\n",
        "        self.top_k = top_k\n",
        "\n",
        "    def build_graph(self, cnv_feat, methy_feat, mrna_feat, mirna_feat):\n",
        "        all_features = np.concatenate([cnv_feat, methy_feat, mrna_feat, mirna_feat])\n",
        "        num_nodes = len(all_features)\n",
        "\n",
        "        cnv_size = len(cnv_feat)\n",
        "        methy_size = len(methy_feat)\n",
        "        mrna_size = len(mrna_feat)\n",
        "        mirna_size = len(mirna_feat)\n",
        "\n",
        "        edge_index = self._build_knn_edges(all_features, self.top_k)\n",
        "\n",
        "        cross_edges = self._build_cross_omics_edges(\n",
        "            cnv_size, methy_size, mrna_size, mirna_size\n",
        "        )\n",
        "        if cross_edges.size > 0:\n",
        "            edge_index = np.concatenate([edge_index, cross_edges], axis=1)\n",
        "\n",
        "        x = torch.FloatTensor(all_features.reshape(-1, 1))\n",
        "        edge_index = torch.LongTensor(edge_index)\n",
        "\n",
        "        return x, edge_index\n",
        "\n",
        "    def _build_knn_edges(self, features, k):\n",
        "        num_nodes = len(features)\n",
        "        features_reshaped = features.reshape(-1, 1)\n",
        "\n",
        "        distances = np.abs(features_reshaped - features_reshaped.T)\n",
        "\n",
        "        edges = []\n",
        "        for i in range(num_nodes):\n",
        "            nearest = np.argsort(distances[i])[:k+1]\n",
        "            for j in nearest:\n",
        "                if i != j:\n",
        "                    edges.append([i, j])\n",
        "\n",
        "        if len(edges) == 0:\n",
        "            edges = [[i, i+1] for i in range(num_nodes-1)]\n",
        "\n",
        "        return np.array(edges).T\n",
        "\n",
        "    def _build_cross_omics_edges(self, cnv_size, methy_size, mrna_size, mirna_size):\n",
        "        edges = []\n",
        "\n",
        "        offsets = [0, cnv_size, cnv_size + methy_size, cnv_size + methy_size + mrna_size]\n",
        "        sizes = [cnv_size, methy_size, mrna_size, mirna_size]\n",
        "\n",
        "        for i in range(len(offsets)):\n",
        "            for j in range(i+1, len(offsets)):\n",
        "                num_cross_edges = min(5, sizes[i], sizes[j])\n",
        "                if num_cross_edges <= 0:\n",
        "                    continue\n",
        "                for k in range(num_cross_edges):\n",
        "                    src = offsets[i] + int(k * (sizes[i] / num_cross_edges))\n",
        "                    dst = offsets[j] + int(k * (sizes[j] / num_cross_edges))\n",
        "                    edges.append([src, dst])\n",
        "                    edges.append([dst, src])\n",
        "\n",
        "        if len(edges) == 0:\n",
        "            edges = [[0, 1]]\n",
        "\n",
        "        return np.array(edges).T"
      ],
      "metadata": {
        "id": "sdouhoDX1AjR"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiOmicsGraphDataset(Dataset):\n",
        "    def __init__(self, cnv, methy, mrna, mirna, labels, indices, graph_builder, scalers=None):\n",
        "        self.cnv = cnv.values[indices]\n",
        "        self.methy = methy.values[indices]\n",
        "        self.mrna = mrna.values[indices]\n",
        "        self.mirna = mirna.values[indices]\n",
        "        self.labels = labels.values[indices].ravel()\n",
        "        self.graph_builder = graph_builder\n",
        "\n",
        "        self.labels = self.labels.astype(int)\n",
        "\n",
        "        if scalers is None:\n",
        "            self.scaler_cnv = StandardScaler().fit(self.cnv)\n",
        "            self.scaler_methy = StandardScaler().fit(self.methy)\n",
        "            self.scaler_mrna = StandardScaler().fit(self.mrna)\n",
        "            self.scaler_mirna = StandardScaler().fit(self.mirna)\n",
        "        else:\n",
        "            self.scaler_cnv, self.scaler_methy, self.scaler_mrna, self.scaler_mirna = scalers\n",
        "\n",
        "        self.cnv = self.scaler_cnv.transform(self.cnv)\n",
        "        self.methy = self.scaler_methy.transform(self.methy)\n",
        "        self.mrna = self.scaler_mrna.transform(self.mrna)\n",
        "        self.mirna = self.scaler_mirna.transform(self.mirna)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        x, edge_index = self.graph_builder.build_graph(\n",
        "            self.cnv[idx],\n",
        "            self.methy[idx],\n",
        "            self.mrna[idx],\n",
        "            self.mirna[idx]\n",
        "        )\n",
        "\n",
        "        label = int(self.labels[idx])\n",
        "        label = torch.LongTensor([label])[0]\n",
        "\n",
        "        data = Data(x=x, edge_index=edge_index, y=label)\n",
        "\n",
        "        return data\n",
        "\n",
        "    def get_scalers(self):\n",
        "        return (self.scaler_cnv, self.scaler_methy, self.scaler_mrna, self.scaler_mirna)\n",
        "\n",
        "def collate_fn(batch):\n",
        "    return Batch.from_data_list(batch)"
      ],
      "metadata": {
        "id": "uOlxpr301bCD"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ProjectionHead(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(input_dim, hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_dim, output_dim)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return F.normalize(self.net(x), dim=1)\n",
        "\n",
        "class MultiOmicsGAT(nn.Module):\n",
        "\n",
        "    def __init__(self, input_dim, hidden_dim, num_classes, num_gat_layers=3,\n",
        "                 heads=4, dropout=0.3, projection_dim=128):\n",
        "        super().__init__()\n",
        "\n",
        "        self.input_proj = nn.Linear(input_dim, hidden_dim)\n",
        "\n",
        "        self.gat_layers = nn.ModuleList()\n",
        "        for i in range(num_gat_layers):\n",
        "            self.gat_layers.append(\n",
        "                GATConv(hidden_dim, hidden_dim // heads, heads=heads, dropout=dropout)\n",
        "            )\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(hidden_dim * 2, hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(hidden_dim, num_classes)\n",
        "        )\n",
        "\n",
        "        self.projection = ProjectionHead(hidden_dim * 2, hidden_dim, projection_dim)\n",
        "\n",
        "    def forward(self, data, return_embedding=False):\n",
        "        x, edge_index, batch = data.x, data.edge_index, data.batch\n",
        "\n",
        "        x = self.input_proj(x)\n",
        "        x = F.relu(x)\n",
        "        for gat in self.gat_layers:\n",
        "            x = gat(x, edge_index)\n",
        "            x = F.elu(x)\n",
        "            x = self.dropout(x)\n",
        "\n",
        "        x_mean = global_mean_pool(x, batch)\n",
        "        x_max = global_max_pool(x, batch)\n",
        "        x = torch.cat([x_mean, x_max], dim=1)\n",
        "\n",
        "        if return_embedding:\n",
        "            return x\n",
        "\n",
        "        out = self.classifier(x)\n",
        "\n",
        "        z = self.projection(x)\n",
        "\n",
        "        return out, z\n"
      ],
      "metadata": {
        "id": "C-0BXfQQ1hD4"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class NTXentLoss(nn.Module):\n",
        "    def __init__(self, temperature=0.07):\n",
        "        super().__init__()\n",
        "        self.temperature = temperature\n",
        "\n",
        "    def forward(self, z):\n",
        "        batch_size = z.shape[0]\n",
        "\n",
        "        sim_matrix = torch.matmul(z, z.T) / self.temperature\n",
        "        mask = torch.eye(batch_size, device=z.device).bool()\n",
        "        sim_matrix.masked_fill_(mask, -9e15)\n",
        "\n",
        "        exp_sim = torch.exp(sim_matrix)\n",
        "        loss = -torch.log(exp_sim / exp_sim.sum(dim=1, keepdim=True)).mean()\n",
        "\n",
        "        return loss\n"
      ],
      "metadata": {
        "id": "15K3cD8J1xDt"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Trainer:\n",
        "    def __init__(self, model, train_loader, val_loader, config):\n",
        "        self.model = model.to(config.device)\n",
        "        self.train_loader = train_loader\n",
        "        self.val_loader = val_loader\n",
        "        self.config = config\n",
        "\n",
        "        self.optimizer = torch.optim.Adam(model.parameters(), lr=config.learning_rate)\n",
        "        self.scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
        "            self.optimizer, mode='min', factor=0.5, patience=5\n",
        "        )\n",
        "\n",
        "        self.ce_loss = nn.CrossEntropyLoss()\n",
        "        self.contrastive_loss = NTXentLoss(temperature=config.temperature)\n",
        "\n",
        "        self.best_val_loss = float('inf')\n",
        "        self.patience_counter = 0\n",
        "\n",
        "    def train_epoch(self):\n",
        "        self.model.train()\n",
        "        total_loss = 0\n",
        "        all_preds = []\n",
        "        all_labels = []\n",
        "\n",
        "        for batch in tqdm(self.train_loader, desc='Training'):\n",
        "            batch = batch.to(self.config.device)\n",
        "\n",
        "            self.optimizer.zero_grad()\n",
        "            logits, z = self.model(batch)\n",
        "\n",
        "            ce_loss = self.ce_loss(logits, batch.y)\n",
        "            cont_loss = self.contrastive_loss(z)\n",
        "\n",
        "            loss = ce_loss + self.config.contrastive_weight * cont_loss\n",
        "\n",
        "            loss.backward()\n",
        "            torch.nn.utils.clip_grad_norm_(self.model.parameters(), 1.0)\n",
        "            self.optimizer.step()\n",
        "\n",
        "            total_loss += loss.item()\n",
        "\n",
        "            preds = torch.argmax(logits, dim=1).cpu().numpy()\n",
        "            all_preds.extend(preds)\n",
        "            all_labels.extend(batch.y.cpu().numpy())\n",
        "\n",
        "        avg_loss = total_loss / len(self.train_loader)\n",
        "        accuracy = accuracy_score(all_labels, all_preds)\n",
        "\n",
        "        return avg_loss, accuracy\n",
        "\n",
        "    def validate(self):\n",
        "        self.model.eval()\n",
        "        total_loss = 0\n",
        "        all_preds = []\n",
        "        all_labels = []\n",
        "        all_probs = []\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for batch in self.val_loader:\n",
        "                batch = batch.to(self.config.device)\n",
        "\n",
        "                logits, z = self.model(batch)\n",
        "\n",
        "                ce_loss = self.ce_loss(logits, batch.y)\n",
        "                cont_loss = self.contrastive_loss(z)\n",
        "                loss = ce_loss + self.config.contrastive_weight * cont_loss\n",
        "\n",
        "                total_loss += loss.item()\n",
        "\n",
        "                probs = F.softmax(logits, dim=1)\n",
        "                preds = torch.argmax(logits, dim=1).cpu().numpy()\n",
        "\n",
        "                all_preds.extend(preds)\n",
        "                all_labels.extend(batch.y.cpu().numpy())\n",
        "                all_probs.extend(probs.cpu().numpy())\n",
        "\n",
        "        avg_loss = total_loss / len(self.val_loader)\n",
        "        accuracy = accuracy_score(all_labels, all_preds)\n",
        "\n",
        "        f1 = f1_score(all_labels, all_preds, average='weighted')\n",
        "\n",
        "        try:\n",
        "            all_probs = np.array(all_probs)\n",
        "            if all_probs.shape[1] == 2:\n",
        "                auc = roc_auc_score(all_labels, all_probs[:, 1])\n",
        "            else:\n",
        "                auc = roc_auc_score(all_labels, all_probs, multi_class='ovr')\n",
        "        except:\n",
        "            auc = 0.0\n",
        "\n",
        "        return avg_loss, accuracy, f1, auc\n",
        "\n",
        "    def train(self):\n",
        "\n",
        "        for epoch in range(self.config.num_epochs):\n",
        "            train_loss, train_acc = self.train_epoch()\n",
        "\n",
        "            val_loss, val_acc, val_f1, val_auc = self.validate()\n",
        "\n",
        "            self.scheduler.step(val_loss)\n",
        "\n",
        "            print(f\"Epoch {epoch+1}/{self.config.num_epochs}\")\n",
        "            print(f\"  Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.4f}\")\n",
        "            print(f\"  Val Loss: {val_loss:.4f} | Val Acc: {val_acc:.4f} | Val F1: {val_f1:.4f} | Val AUC: {val_auc:.4f}\")\n",
        "\n",
        "            if val_loss < self.best_val_loss:\n",
        "                self.best_val_loss = val_loss\n",
        "                self.patience_counter = 0\n",
        "                self.save_model()\n",
        "            else:\n",
        "                self.patience_counter += 1\n",
        "                if self.patience_counter >= self.config.patience:\n",
        "                    print(f\"\\nEarly stopping triggered after {epoch+1} epochs\")\n",
        "                    break\n",
        "\n",
        "            print(\"-\" * 70)\n",
        "\n",
        "        print(f\"Best validation loss: {self.best_val_loss:.4f}\")\n",
        "\n",
        "    def save_model(self):\n",
        "        save_path = os.path.join(self.config.save_dir, self.config.model_name)\n",
        "        torch.save({\n",
        "            'model_state_dict': self.model.state_dict(),\n",
        "            'config': self.config,\n",
        "        }, save_path)"
      ],
      "metadata": {
        "id": "GsiTj-nK123L"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def main():\n",
        "    data_loader = TCGADataLoader(config)\n",
        "    cnv, methy, mrna, mirna, labels = data_loader.load_data()\n",
        "\n",
        "    train_idx, test_idx = data_loader.create_train_test_split(\n",
        "        cnv, methy, mrna, mirna, labels\n",
        "    )\n",
        "\n",
        "    print(f\"\\nTrain samples: {len(train_idx)}, Test samples: {len(test_idx)}\")\n",
        "\n",
        "    graph_builder = PatientGraphBuilder(top_k=config.top_k_neighbors)\n",
        "\n",
        "    train_dataset = MultiOmicsGraphDataset(\n",
        "        cnv, methy, mrna, mirna, labels, train_idx, graph_builder\n",
        "    )\n",
        "\n",
        "    scalers = train_dataset.get_scalers()\n",
        "    test_dataset = MultiOmicsGraphDataset(\n",
        "        cnv, methy, mrna, mirna, labels, test_idx, graph_builder, scalers=scalers\n",
        "    )\n",
        "\n",
        "    train_loader = DataLoader(\n",
        "        train_dataset,\n",
        "        batch_size=config.batch_size,\n",
        "        shuffle=True,\n",
        "        collate_fn=collate_fn\n",
        "    )\n",
        "    test_loader = DataLoader(\n",
        "        test_dataset,\n",
        "        batch_size=config.batch_size,\n",
        "        shuffle=False,\n",
        "        collate_fn=collate_fn\n",
        "    )\n",
        "\n",
        "    num_classes = len(np.unique(labels.values))\n",
        "    print(f\"Number of classes: {num_classes}\")\n",
        "\n",
        "    model = MultiOmicsGAT(\n",
        "        input_dim=1,  # Each node has 1 feature (the omics value)\n",
        "        hidden_dim=config.hidden_dim,\n",
        "        num_classes=num_classes,\n",
        "        num_gat_layers=config.num_gat_layers,\n",
        "        heads=config.gat_heads,\n",
        "        dropout=config.dropout,\n",
        "        projection_dim=config.projection_dim\n",
        "    )\n",
        "\n",
        "    total_params = sum(p.numel() for p in model.parameters())\n",
        "    print(f\"Total model parameters: {total_params:,}\")\n",
        "\n",
        "    trainer = Trainer(model, train_loader, test_loader, config)\n",
        "    trainer.train()"
      ],
      "metadata": {
        "id": "Im06Cwoz19zE"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "811fosjO2HAT",
        "outputId": "e6c67396-ef78-48d3-8b82-b45a7214164c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading from: MLOmics/Main_Dataset/Classification_datasets/GS-BRCA/Top\n",
            "Loaded - CNV: (5000, 671), Methy: (5000, 671), mRNA: (5000, 671), miRNA: (366, 671), Labels: (671, 0)\n",
            "Transposing CNV from (features x samples) to (samples x features): (5000, 671) -> (671, 5000)\n",
            "Transposing Methy from (features x samples) to (samples x features): (5000, 671) -> (671, 5000)\n",
            "Transposing mRNA from (features x samples) to (samples x features): (5000, 671) -> (671, 5000)\n",
            "Transposing miRNA from (features x samples) to (samples x features): (366, 671) -> (671, 366)\n",
            "After ensure_samples_rows - CNV: (671, 5000), Methy: (671, 5000), mRNA: (671, 5000), miRNA: (671, 366)\n",
            "Labels processed: (671, 1)\n",
            "\n",
            "Found 671 common samples\n",
            "\n",
            "Label types before encoding: object\n",
            "Unique labels: ['0' '1' '2' '3' '4']\n",
            "Labels are categorical/strings. Encoding to numeric...\n",
            "Label mapping: {'0': np.int64(0), '1': np.int64(1), '2': np.int64(2), '3': np.int64(3), '4': np.int64(4)}\n",
            "Labels after encoding: int64, unique values: [0 1 2 3 4]\n",
            "Final data shapes - CNV: (671, 5000), Methy: (671, 5000), mRNA: (671, 5000), miRNA: (671, 366), Labels: (671, 1)\n",
            "\n",
            "Train samples: 536, Test samples: 135\n",
            "Number of classes: 5\n",
            "Total model parameters: 496,261\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 268/268 [56:58<00:00, 12.75s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "  Train Loss: nan | Train Acc: 0.5261\n",
            "  Val Loss: nan | Val Acc: 0.5259 | Val F1: 0.3625 | Val AUC: 0.0000\n",
            "----------------------------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 268/268 [55:40<00:00, 12.46s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2/100\n",
            "  Train Loss: nan | Train Acc: 0.5261\n",
            "  Val Loss: nan | Val Acc: 0.5259 | Val F1: 0.3625 | Val AUC: 0.0000\n",
            "----------------------------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training:  62%|██████▏   | 167/268 [34:52<20:40, 12.28s/it]"
          ]
        }
      ]
    }
  ]
}